#!/usr/bin/env python3
"""
Lib: PDF Downloader
åŸºäºè¿‡æ»¤ç»“æœä¸‹è½½PDFæ–‡ä»¶
æ”¯æŒOpenReviewå’ŒarXivåŒé€šé“
"""

import json
import time
import requests
import re
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from datetime import datetime


class PDFDownloader:
    """PDFä¸‹è½½å™¨"""
    
    def __init__(self, output_dir: Path, timeout: int = 60):
        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.timeout = timeout
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        self.stats = {
            'total': 0,
            'success': 0,
            'failed': 0,
            'skipped': 0
        }
    
    def download(self, paper: Dict, paper_idx: int, total: int) -> Optional[Path]:
        """ä¸‹è½½å•ç¯‡è®ºæ–‡çš„PDF"""
        paper_id = paper.get('id', 'unknown')
        title = paper.get('title', 'Unknown')
        pdf_url = paper.get('pdf_url')
        
        print(f"\n[{paper_idx}/{total}] {title[:70]}...")
        self.stats['total'] += 1
        
        # ç”Ÿæˆæ–‡ä»¶å
        year = paper.get('year', 2024)
        authors = paper.get('authors', [])
        first_author = authors[0].split()[-1] if authors else 'Unknown'
        safe_title = self._sanitize_filename(title)[:50]
        filename = f"{year}_{first_author}_{safe_title}.pdf"
        
        output_path = self.output_dir / filename
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if output_path.exists():
            print(f"  â­ï¸  å·²å­˜åœ¨ï¼Œè·³è¿‡")
            self.stats['skipped'] += 1
            return output_path
        
        # å°è¯•ä¸‹è½½
        success = False
        
        # 1. å°è¯•OpenReview PDF
        if pdf_url and not success:
            print(f"  ğŸ“¥ å°è¯•OpenReviewä¸‹è½½...")
            success = self._try_download(pdf_url, output_path)
            if success:
                print(f"  âœ“ OpenReviewä¸‹è½½æˆåŠŸ")
        
        # 2. å°è¯•arXivæœç´¢
        if not success:
            print(f"  ğŸ” å°è¯•æœç´¢arXiv...")
            arxiv_url = self._search_arxiv(title)
            if arxiv_url:
                success = self._try_download(arxiv_url, output_path)
                if success:
                    print(f"  âœ“ arXivä¸‹è½½æˆåŠŸ")
        
        if success:
            self.stats['success'] += 1
            return output_path
        else:
            print(f"  âœ— æ‰€æœ‰ä¸‹è½½æ–¹å¼å‡å¤±è´¥")
            self.stats['failed'] += 1
            return None
    
    def _try_download(self, url: str, output_path: Path) -> bool:
        """å°è¯•ä¸‹è½½URLåˆ°æŒ‡å®šè·¯å¾„"""
        try:
            response = self.session.get(url, timeout=self.timeout, stream=True)
            response.raise_for_status()
            
            # æ£€æŸ¥å†…å®¹ç±»å‹
            content_type = response.headers.get('content-type', '')
            if 'pdf' not in content_type.lower() and not url.endswith('.pdf'):
                # å¯èƒ½æ˜¯HTMLé¡µé¢ï¼Œæ£€æŸ¥å†…å®¹
                content = response.content[:100]
                if b'%PDF' not in content:
                    return False
            
            with open(output_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
            
            return True
            
        except Exception as e:
            return False
    
    def _search_arxiv(self, title: str) -> Optional[str]:
        """æ ¹æ®æ ‡é¢˜æœç´¢arXiv"""
        try:
            import feedparser
            
            # ç®€åŒ–æ ‡é¢˜ç”¨äºæœç´¢
            search_title = title.replace(' ', '+')[:100]
            url = f"http://export.arxiv.org/api/query?search_query=ti:{search_title}&max_results=1"
            
            response = requests.get(url, timeout=10)
            feed = feedparser.parse(response.content)
            
            if feed.entries:
                entry = feed.entries[0]
                # æ£€æŸ¥æ ‡é¢˜ç›¸ä¼¼åº¦
                arxiv_title = str(entry.title).lower()
                query_title = title.lower()
                
                # ç®€å•ç›¸ä¼¼åº¦æ£€æŸ¥
                if self._title_similarity(arxiv_title, query_title) > 0.6:
                    for link in entry.links:
                        if link.get('title') == 'pdf':
                            return str(link.href)
            
            return None
            
        except Exception as e:
            return None
    
    def _title_similarity(self, title1: str, title2: str) -> float:
        """è®¡ç®—æ ‡é¢˜ç›¸ä¼¼åº¦"""
        words1 = set(title1.lower().split())
        words2 = set(title2.lower().split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = words1 & words2
        union = words1 | words2
        
        return len(intersection) / len(union) if union else 0.0
    
    def _sanitize_filename(self, filename: str) -> str:
        """æ¸…ç†æ–‡ä»¶å"""
        filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
        return filename.strip()
    
    def get_stats(self) -> Dict:
        """è·å–ä¸‹è½½ç»Ÿè®¡"""
        return self.stats.copy()


def generate_bibtex(papers: List[Dict]) -> str:
    """ç”ŸæˆBibTeXå¼•ç”¨"""
    bibtex_entries = []
    
    for paper in papers:
        title = paper.get('title', '')
        authors = paper.get('authors', [])
        year = paper.get('year', 2024)
        venue = paper.get('venue_id', '')
        paper_id = paper.get('id', '')
        
        # ç”Ÿæˆå¼•ç”¨é”®
        first_author = authors[0].split()[-1] if authors else 'Unknown'
        cite_key = f"{first_author.lower()}{year}{title.split()[0].lower()}"
        cite_key = ''.join(c for c in cite_key if c.isalnum())
        
        # æ ¼å¼åŒ–ä½œè€…
        author_str = ' and '.join(authors) if authors else 'Unknown'
        
        # æå–ä¼šè®®åç§°
        venue_name = venue.split('/')[0] if '/' in venue else venue
        
        entry = f"""@inproceedings{{{cite_key},
  title = {{{title}}},
  author = {{{author_str}}},
  booktitle = {{{venue_name}}},
  year = {{{year}}},
  url = {{https://openreview.net/forum?id={paper_id}}}
}}"""
        
        bibtex_entries.append(entry)
    
    return '\n\n'.join(bibtex_entries)


def download_papers_from_file(
    input_file: Path,
    output_dir: Path,
    delay: float = 1.0
) -> Tuple[List[Dict], Path]:
    """
    ä»æ–‡ä»¶ä¸‹è½½è®ºæ–‡PDF
    
    Returns:
        (å·²ä¸‹è½½è®ºæ–‡åˆ—è¡¨, è¾“å‡ºç›®å½•)
    """
    print(f"\n{'='*60}")
    print(f"ğŸ“¥ é˜¶æ®µ3: PDFä¸‹è½½")
    print(f"{'='*60}")
    print(f"è¾“å…¥æ–‡ä»¶: {input_file}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ä¸‹è½½å»¶è¿Ÿ: {delay}ç§’")
    print(f"{'='*60}\n")
    
    # è¯»å–è¾“å…¥æ–‡ä»¶
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # è·å–è®ºæ–‡åˆ—è¡¨
    if 'relevant_papers' in data:
        papers = data['relevant_papers']
    elif 'papers' in data:
        papers = data['papers']
    else:
        papers = []
    
    # è¿‡æ»¤å‡ºéœ€è¦ä¸‹è½½çš„è®ºæ–‡
    papers_to_download = [p for p in papers if p.get('relevant', True)]
    
    print(f"ğŸ“š æ‰¾åˆ° {len(papers_to_download)} ç¯‡ç›¸å…³è®ºæ–‡")
    
    if not papers_to_download:
        print("âš ï¸ æ²¡æœ‰éœ€è¦ä¸‹è½½çš„è®ºæ–‡")
        return [], output_dir
    
    # åˆ›å»ºä¸‹è½½å™¨
    output_dir.mkdir(parents=True, exist_ok=True)
    downloader = PDFDownloader(output_dir)
    
    # ä¸‹è½½è®ºæ–‡
    downloaded_papers = []
    for idx, paper in enumerate(papers_to_download, 1):
        pdf_path = downloader.download(paper, idx, len(papers_to_download))
        
        if pdf_path:
            paper['local_pdf'] = str(pdf_path)
            downloaded_papers.append(paper)
        
        # å»¶è¿Ÿ
        if idx < len(papers_to_download):
            time.sleep(delay)
    
    # ç”ŸæˆBibTeX
    print(f"\nğŸ“ ç”ŸæˆBibTeX...")
    bibtex_content = generate_bibtex(downloaded_papers)
    bibtex_path = output_dir / "references.bib"
    with open(bibtex_path, 'w', encoding='utf-8') as f:
        f.write(bibtex_content)
    
    # ä¿å­˜ä¸‹è½½è®°å½•
    record_path = output_dir / "download_record.json"
    with open(record_path, 'w', encoding='utf-8') as f:
        json.dump({
            'download_time': datetime.now().isoformat(),
            'source_file': str(input_file),
            'total_papers': len(papers_to_download),
            'downloaded': len(downloaded_papers),
            'stats': downloader.get_stats(),
            'papers': downloaded_papers
        }, f, ensure_ascii=False, indent=2)
    
    # æ‰“å°ç»Ÿè®¡
    stats = downloader.get_stats()
    print(f"\n{'='*60}")
    print(f"âœ… ä¸‹è½½å®Œæˆ!")
    print(f"ğŸ“Š ç»Ÿè®¡:")
    print(f"   æ€»è®¡: {stats['total']}")
    print(f"   æˆåŠŸ: {stats['success']}")
    print(f"   å¤±è´¥: {stats['failed']}")
    print(f"   è·³è¿‡: {stats['skipped']}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ğŸ“š BibTeX: {bibtex_path}")
    print(f"{'='*60}\n")
    
    return downloaded_papers, output_dir
