#!/usr/bin/env python3
"""
Lib: OpenReview Crawler
çˆ¬å–æŒ‡å®šä¼šè®®å’Œå¹´ä»½çš„æ‰€æœ‰è®ºæ–‡ä¿¡æ¯
"""

import json
import time
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional, Tuple, Any


def get_openreview_client() -> Tuple[Any, str]:
    """è·å–OpenReviewå®¢æˆ·ç«¯"""
    try:
        import openreview
        # å°è¯•ä½¿ç”¨API V2
        try:
            client = openreview.api.OpenReviewClient(
                baseurl='https://api2.openreview.net'
            )
            return client, 'v2'
        except:
            # å›é€€åˆ°API V1
            client = openreview.Client(
                baseurl='https://api.openreview.net'
            )
            return client, 'v1'
    except ImportError:
        print("é”™è¯¯: è¯·å®‰è£…openreview-py: pip install openreview-py")
        raise RuntimeError("openreview-py not installed")


def crawl_conference(client, venue_id: str, api_version: str, accepted_only: bool = True) -> List[Dict]:
    """
    çˆ¬å–å•ä¸ªä¼šè®®çš„æ‰€æœ‰è®ºæ–‡
    
    Args:
        client: OpenReviewå®¢æˆ·ç«¯
        venue_id: ä¼šè®®ID, å¦‚ "ICLR.cc/2024/Conference"
        api_version: APIç‰ˆæœ¬ 'v1' æˆ– 'v2'
        accepted_only: æ˜¯å¦åªè·å–å·²æ¥å—è®ºæ–‡
    
    Returns:
        è®ºæ–‡åˆ—è¡¨
    """
    papers = []
    
    try:
        print(f"  è·å–ä¼šè®®ä¿¡æ¯: {venue_id}")
        venue_group = client.get_group(venue_id)
        
        # è·å–æäº¤åç§°
        if api_version == 'v2':
            submission_name = venue_group.content.get('submission_name', {}).get('value', 'Submission')
        else:
            submission_name = 'Submission'
        
        print(f"  è·å–æ‰€æœ‰æäº¤è®ºæ–‡...")
        submissions = client.get_all_notes(
            invitation=f'{venue_id}/-/{submission_name}',
            details='directReplies'
        )
        
        print(f"  æ‰¾åˆ° {len(submissions)} ç¯‡è®ºæ–‡")
        
        for idx, paper in enumerate(submissions, 1):
            if idx % 50 == 0:
                print(f"    å¤„ç†ä¸­... {idx}/{len(submissions)}")
            
            # æå–åŸºæœ¬ä¿¡æ¯
            paper_id = paper.id
            
            if api_version == 'v2':
                title = paper.content.get('title', {}).get('value', '')
                abstract = paper.content.get('abstract', {}).get('value', '')
                authors = paper.content.get('authors', {}).get('value', [])
                keywords = paper.content.get('keywords', {}).get('value', [])
                venue_status = paper.content.get('venue', {}).get('value', '')
                pdf_value = paper.content.get('pdf', {}).get('value', '')
            else:
                title = paper.content.get('title', '')
                abstract = paper.content.get('abstract', '')
                authors = paper.content.get('authors', [])
                keywords = paper.content.get('keywords', [])
                venue_status = paper.content.get('venue', '')
                pdf_value = paper.content.get('pdf', '')
            
            # æ„å»ºPDF URL
            pdf_url = None
            if pdf_value:
                if pdf_value.startswith('/pdf'):
                    pdf_url = f"https://openreview.net{pdf_value}"
                elif pdf_value.startswith('http'):
                    pdf_url = pdf_value
                else:
                    pdf_url = f"https://openreview.net/pdf?id={paper_id}"
            
            # åˆ¤æ–­æ¥å—çŠ¶æ€
            decision = 'unknown'
            if venue_status:
                if any(x in venue_status.lower() for x in ['accept', 'oral', 'poster', 'spotlight']):
                    decision = 'accepted'
                elif 'reject' in venue_status.lower():
                    decision = 'rejected'
            
            # å¦‚æœåªæ¥å—å·²æ¥å—è®ºæ–‡ï¼Œè·³è¿‡æ‹’ç»çš„
            if accepted_only and decision == 'rejected':
                continue
            
            papers.append({
                'id': paper_id,
                'title': title,
                'abstract': abstract,
                'authors': authors,
                'keywords': keywords if isinstance(keywords, list) else [keywords] if keywords else [],
                'venue_id': venue_id,
                'venue_status': venue_status,
                'decision': decision,
                'pdf_url': pdf_url,
                'forum': paper.forum,
                'cdate': paper.cdate,
                'mdate': paper.mdate
            })
            
            # é¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(0.01)
        
        return papers
        
    except Exception as e:
        print(f"  é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return []


def crawl_venues(venues: List[str], output_dir: Path, accepted_only: bool = True) -> Tuple[List[Dict], Optional[Path]]:
    """
    çˆ¬å–å¤šä¸ªä¼šè®®çš„è®ºæ–‡
    
    Returns:
        (æ‰€æœ‰è®ºæ–‡åˆ—è¡¨, æ±‡æ€»æ–‡ä»¶è·¯å¾„)
    """
    client, api_version = get_openreview_client()
    if not client:
        return [], None
    
    print(f"\n{'='*60}")
    print(f"ğŸ•·ï¸ é˜¶æ®µ1: OpenReviewçˆ¬è™«")
    print(f"{'='*60}")
    print(f"APIç‰ˆæœ¬: {api_version}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print(f"åªæ¥å—å·²æ¥å—è®ºæ–‡: {accepted_only}")
    print(f"{'='*60}\n")
    
    output_dir.mkdir(parents=True, exist_ok=True)
    all_papers = []
    
    for venue in venues:
        print(f"\nğŸ“š æ­£åœ¨çˆ¬å–: {venue}")
        papers = crawl_conference(client, venue, api_version, accepted_only)
        
        if papers:
            # æå–å¹´ä»½
            year = 2024  # é»˜è®¤
            for part in venue.split('/'):
                if part.isdigit():
                    year = int(part)
                    break
            
            # å•ç‹¬ä¿å­˜æ¯ä¸ªä¼šè®®
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            venue_name = venue.replace('/', '_').replace('.', '_')
            filename = f"{venue_name}_{year}_{timestamp}.json"
            output_path = output_dir / filename
            
            data = {
                'venue': venue,
                'year': year,
                'crawl_time': datetime.now().isoformat(),
                'total_papers': len(papers),
                'papers': papers
            }
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            print(f"  âœ“ å·²ä¿å­˜: {output_path} ({len(papers)}ç¯‡)")
            all_papers.extend(papers)
        else:
            print(f"  âš ï¸ æœªæ‰¾åˆ°è®ºæ–‡æˆ–çˆ¬å–å¤±è´¥")
        
        # ä¼šè®®é—´å»¶è¿Ÿ
        time.sleep(2)
    
    # ä¿å­˜æ±‡æ€»æ–‡ä»¶
    if all_papers:
        summary_path = output_dir / f"all_papers_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump({
                'crawl_time': datetime.now().isoformat(),
                'total_papers': len(all_papers),
                'venues': venues,
                'papers': all_papers
            }, f, ensure_ascii=False, indent=2)
        
        print(f"\n{'='*60}")
        print(f"âœ… çˆ¬å–å®Œæˆ!")
        print(f"ğŸ“Š æ€»è®¡: {len(all_papers)} ç¯‡è®ºæ–‡")
        print(f"ğŸ“ æ±‡æ€»æ–‡ä»¶: {summary_path}")
        print(f"{'='*60}\n")
        
        return all_papers, summary_path
    
    return [], None
