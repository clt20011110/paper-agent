#!/usr/bin/env python3
"""
Lib: Keyword Filter
åŸºäºå…³é”®è¯çš„è®ºæ–‡è¿‡æ»¤å™¨
æ”¯æŒåŒä¹‰è¯æ‰©å±•ã€AND/OR/NOTé€»è¾‘
"""

import json
import re
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Set, Optional, Tuple
from dataclasses import dataclass, field


# å†…ç½®åŒä¹‰è¯åº“
BUILTIN_SYNONYMS = {
    # åˆ†å­/è¯ç‰©ç›¸å…³
    'molecular': ['molecule', 'molecules', 'chemical', 'chemistry'],
    'drug': ['pharmaceutical', 'medication', 'medicine', 'therapeutic'],
    'compound': ['chemical compound', 'small molecule'],
    'generation': ['generative', 'generating', 'synthesis', 'synthesizing'],
    'design': ['designing', 'discovery', 'screening'],
    
    # AI/MLç›¸å…³
    'diffusion': ['diffusion model', 'diffusion-based'],
    'transformer': ['attention', 'self-attention'],
    'graph': ['graph neural network', 'gnn', 'graph-based'],
    'learning': ['machine learning', 'ml', 'deep learning'],
    
    # ç”Ÿç‰©ç›¸å…³
    'protein': ['proteins', 'peptide', 'amino acid'],
    'binding': ['affinity', 'docking', 'interaction'],
    'target': ['receptor', 'enzyme', 'protein target'],
}


@dataclass
class FilterConfig:
    """è¿‡æ»¤é…ç½®"""
    include_groups: List[List[str]] = field(default_factory=list)  # OR of AND groups
    exclude: List[str] = field(default_factory=list)
    synonyms: Dict[str, List[str]] = field(default_factory=dict)
    match_fields: List[str] = field(default_factory=lambda: ['title', 'abstract'])
    case_sensitive: bool = False
    whole_word: bool = False
    
    def __post_init__(self):
        """åˆå§‹åŒ–ååˆå¹¶å†…ç½®åŒä¹‰è¯"""
        # åˆå¹¶ç”¨æˆ·åŒä¹‰è¯å’Œå†…ç½®åŒä¹‰è¯
        merged = BUILTIN_SYNONYMS.copy()
        merged.update(self.synonyms)
        self.synonyms = merged


class KeywordFilter:
    """å…³é”®è¯è¿‡æ»¤å™¨"""
    
    def __init__(self, config: FilterConfig):
        self.config = config
        self._compile_patterns()
    
    def _compile_patterns(self):
        """é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ä»¥æé«˜æ€§èƒ½"""
        self.patterns = {}
        
        for word in self._get_all_keywords():
            flags = 0 if self.config.case_sensitive else re.IGNORECASE
            
            if self.config.whole_word:
                pattern = rf'\b{re.escape(word)}\b'
            else:
                pattern = rf'{re.escape(word)}'
            
            self.patterns[word] = re.compile(pattern, flags)
    
    def _get_all_keywords(self) -> Set[str]:
        """è·å–æ‰€æœ‰å…³é”®è¯ï¼ˆåŒ…æ‹¬åŒä¹‰è¯ï¼‰"""
        keywords = set()
        
        # ä»include_groupsæ”¶é›†
        for group in self.config.include_groups:
            for word in group:
                keywords.add(word.lower() if not self.config.case_sensitive else word)
                # æ·»åŠ åŒä¹‰è¯
                if word in self.config.synonyms:
                    for syn in self.config.synonyms[word]:
                        keywords.add(syn.lower() if not self.config.case_sensitive else syn)
        
        # ä»excludeæ”¶é›†
        for word in self.config.exclude:
            keywords.add(word.lower() if not self.config.case_sensitive else word)
        
        return keywords
    
    def _preprocess_text(self, text: str) -> str:
        """é¢„å¤„ç†æ–‡æœ¬"""
        if not text:
            return ""
        
        if not self.config.case_sensitive:
            text = text.lower()
        
        # ç§»é™¤å¤šä½™ç©ºç™½ï¼Œä½†ä¿ç•™å•è¯è¾¹ç•Œ
        text = re.sub(r'\s+', ' ', text)
        
        return text
    
    def _contains_keyword(self, text: str, keyword: str) -> bool:
        """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«å…³é”®è¯"""
        pattern = self.patterns.get(keyword)
        if not pattern:
            return False
        
        return bool(pattern.search(text))
    
    def _get_paper_text(self, paper: Dict) -> str:
        """è·å–è®ºæ–‡çš„å¾…åŒ¹é…æ–‡æœ¬"""
        texts = []
        for field in self.config.match_fields:
            value = paper.get(field, '')
            if isinstance(value, str):
                texts.append(value)
            elif isinstance(value, list):
                texts.append(' '.join(str(v) for v in value))
        return self._preprocess_text(' '.join(texts))
    
    def filter_papers(self, papers: List[Dict]) -> Tuple[List[Dict], List[Dict]]:
        """
        è¿‡æ»¤è®ºæ–‡åˆ—è¡¨
        
        Returns:
            (ç›¸å…³è®ºæ–‡åˆ—è¡¨, ä¸ç›¸å…³è®ºæ–‡åˆ—è¡¨)
        """
        relevant = []
        irrelevant = []
        
        for paper in papers:
            paper_text = self._get_paper_text(paper)
            match_info = self._check_paper(paper_text)
            
            # æ·»åŠ åŒ¹é…ä¿¡æ¯åˆ°è®ºæ–‡
            paper_copy = paper.copy()
            paper_copy['relevant'] = match_info['is_relevant']
            paper_copy['match_info'] = match_info
            
            if match_info['is_relevant']:
                relevant.append(paper_copy)
            else:
                irrelevant.append(paper_copy)
        
        return relevant, irrelevant
    
    def _check_paper(self, text: str) -> Dict:
        """æ£€æŸ¥å•ç¯‡è®ºæ–‡çš„åŒ¹é…æƒ…å†µ"""
        match_info = {
            'is_relevant': False,
            'matched_groups': [],
            'matched_keywords': [],
            'excluded': False,
            'exclude_reason': None
        }
        
        # é¦–å…ˆæ£€æŸ¥æ’é™¤è¯
        for exclude_word in self.config.exclude:
            # æ£€æŸ¥è¯æœ¬èº«
            if self._contains_keyword(text, exclude_word):
                match_info['is_relevant'] = False
                match_info['excluded'] = True
                match_info['exclude_reason'] = f"åŒ…å«æ’é™¤è¯: {exclude_word}"
                return match_info
            
            # æ£€æŸ¥åŒä¹‰è¯
            if exclude_word in self.config.synonyms:
                for syn in self.config.synonyms[exclude_word]:
                    if self._contains_keyword(text, syn):
                        match_info['is_relevant'] = False
                        match_info['excluded'] = True
                        match_info['exclude_reason'] = f"åŒ…å«æ’é™¤è¯åŒä¹‰è¯: {syn} (æ¥è‡ª {exclude_word})"
                        return match_info
        
        # æ£€æŸ¥åŒ…å«ç»„ï¼ˆORé€»è¾‘ï¼‰
        for group_idx, group in enumerate(self.config.include_groups):
            group_matches = []
            all_match = True
            
            for keyword in group:
                keyword_matched = False
                matched_words = []
                
                # æ£€æŸ¥å…³é”®è¯æœ¬èº«
                if self._contains_keyword(text, keyword):
                    keyword_matched = True
                    matched_words.append(keyword)
                
                # æ£€æŸ¥åŒä¹‰è¯
                if keyword in self.config.synonyms:
                    for syn in self.config.synonyms[keyword]:
                        if self._contains_keyword(text, syn):
                            keyword_matched = True
                            matched_words.append(f"{syn}({keyword})")
                
                if keyword_matched:
                    group_matches.extend(matched_words)
                else:
                    all_match = False
                    break
            
            if all_match:
                match_info['is_relevant'] = True
                match_info['matched_groups'].append({
                    'group_index': group_idx,
                    'group': group,
                    'matched_keywords': group_matches
                })
                match_info['matched_keywords'].extend(group_matches)
        
        # å»é‡åŒ¹é…çš„å…³é”®è¯
        match_info['matched_keywords'] = list(set(match_info['matched_keywords']))
        
        return match_info
    
    def get_statistics(self, relevant: List[Dict], irrelevant: List[Dict]) -> Dict:
        """è·å–è¿‡æ»¤ç»Ÿè®¡ä¿¡æ¯"""
        total = len(relevant) + len(irrelevant)
        
        # ç»Ÿè®¡åŒ¹é…ç»„ä½¿ç”¨æƒ…å†µ
        group_counts = {}
        for paper in relevant:
            for group_match in paper.get('match_info', {}).get('matched_groups', []):
                group_key = tuple(group_match['group'])
                group_counts[group_key] = group_counts.get(group_key, 0) + 1
        
        # ç»Ÿè®¡æ’é™¤åŸå› 
        exclude_reasons = {}
        for paper in irrelevant:
            reason = paper.get('match_info', {}).get('exclude_reason', 'æœªåŒ¹é…ä»»ä½•åŒ…å«ç»„')
            exclude_reasons[reason] = exclude_reasons.get(reason, 0) + 1
        
        return {
            'total_papers': total,
            'relevant_count': len(relevant),
            'irrelevant_count': len(irrelevant),
            'relevance_rate': len(relevant) / total * 100 if total > 0 else 0,
            'group_matches': {str(k): v for k, v in group_counts.items()},
            'exclusion_reasons': exclude_reasons
        }


def filter_papers_from_file(
    input_file: Path, 
    output_file: Path,
    config: FilterConfig
) -> Tuple[List[Dict], Dict]:
    """
    ä»æ–‡ä»¶è¿‡æ»¤è®ºæ–‡
    
    Returns:
        (ç›¸å…³è®ºæ–‡åˆ—è¡¨, ç»Ÿè®¡ä¿¡æ¯)
    """
    print(f"\n{'='*60}")
    print(f"ğŸ” é˜¶æ®µ2: å…³é”®è¯è¿‡æ»¤")
    print(f"{'='*60}")
    print(f"è¾“å…¥æ–‡ä»¶: {input_file}")
    print(f"è¾“å‡ºæ–‡ä»¶: {output_file}")
    print(f"{'='*60}\n")
    
    # è¯»å–è®ºæ–‡
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    papers = data.get('papers', [])
    print(f"ğŸ“š è¯»å–åˆ° {len(papers)} ç¯‡è®ºæ–‡")
    
    # åˆ›å»ºè¿‡æ»¤å™¨å¹¶æ‰§è¡Œè¿‡æ»¤
    filter_obj = KeywordFilter(config)
    relevant, irrelevant = filter_obj.filter_papers(papers)
    
    # è·å–ç»Ÿè®¡
    stats = filter_obj.get_statistics(relevant, irrelevant)
    
    # ä¿å­˜ç»“æœ
    output_data = {
        'filter_time': datetime.now().isoformat(),
        'source_file': str(input_file),
        'config': {
            'include_groups': config.include_groups,
            'exclude': config.exclude,
            'synonyms_used': list(config.synonyms.keys()),
            'match_fields': config.match_fields
        },
        'statistics': stats,
        'total_papers': len(papers),
        'relevant_count': len(relevant),
        'irrelevant_count': len(irrelevant),
        'relevant_papers': relevant,
        'irrelevant_papers': irrelevant
    }
    
    output_file.parent.mkdir(parents=True, exist_ok=True)
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    # æ‰“å°ç»Ÿè®¡
    print(f"\nğŸ“Š è¿‡æ»¤ç»Ÿè®¡:")
    print(f"   æ€»è®ºæ–‡æ•°: {stats['total_papers']}")
    print(f"   ç›¸å…³è®ºæ–‡: {stats['relevant_count']} ({stats['relevance_rate']:.1f}%)")
    print(f"   ä¸ç›¸å…³è®ºæ–‡: {stats['irrelevant_count']}")
    
    if stats['group_matches']:
        print(f"\nğŸ·ï¸  åŒ¹é…ç»„ç»Ÿè®¡:")
        for group, count in stats['group_matches'].items():
            print(f"   {group}: {count}ç¯‡")
    
    if stats['exclusion_reasons']:
        print(f"\nğŸš« æ’é™¤ç»Ÿè®¡:")
        for reason, count in list(stats['exclusion_reasons'].items())[:5]:
            print(f"   {reason}: {count}ç¯‡")
    
    print(f"\nâœ… ç»“æœå·²ä¿å­˜: {output_file}")
    
    return relevant, stats
